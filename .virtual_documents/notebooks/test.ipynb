import platform, sys, os
PATH = os.environ['PATH']
from pygmtsar import __version__
print(__version__)

import xarray as xr
import numpy as np
import pandas as pd
import geopandas as gpd
import json
from dask.distributed import Client
import dask
import warnings
warnings.filterwarnings('ignore')

# plotting modules
import pyvista as pv

from pygmtsar import S1, Stack, tqdm_dask, ASF, Tiles, XYZTiles, utils

# magic trick for white background
pv.set_plot_theme("document")
import panel
panel.extension(comms='ipywidgets')
panel.extension('vtk')
from contextlib import contextmanager
import matplotlib.pyplot as plt
@contextmanager
def mpl_settings(settings):
    original_settings = {k: plt.rcParams[k] for k in settings}
    plt.rcParams.update(settings)
    yield
    plt.rcParams.update(original_settings)
plt.rcParams['figure.figsize'] = [12, 4]
plt.rcParams['figure.dpi'] = 150
plt.rcParams['figure.titlesize'] = 24
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12



# define Pandas display settings
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 100)

#%matplotlib inline


import platform, sys, os
PATH = os.environ['PATH']
from pygmtsar import __version__
import xarray as xr
import numpy as np
import pandas as pd
import geopandas as gpd
import json
from dask.distributed import Client
import dask
import warnings
warnings.filterwarnings('ignore')
import pickle

# plotting modules
import pyvista as pv

from pygmtsar import S1, Stack, tqdm_dask, ASF, Tiles, XYZTiles, utils

from os.path import abspath, dirname, join
PROJ_PATH = '/home/rav_marcin/projects/msg2sar'
sys.path.insert(0, PROJ_PATH)

from settings.paths import setup
setup()


from settings.paths import DATA_DIR
from utils.internal.io.json_io import open_json
from settings.paths import KEYS_DIR
from utils.internal.io.s1_stack import init_stack
from utils.internal.geo.aoi import get_aoi


main_folder = join(DATA_DIR, 'sar/sbas/desc/2023/bogo_pl_test')
WORKDIR = os.path.join(main_folder, 'raw')
DATADIR = os.path.join(main_folder, 'data')

# The subswath is required for partial scene downloads and is not used for burst downloads.
# The orbit is used to define directory names.
ORBIT    = 'D'
SUBSWATH = 3
REFERENCE = '2023-02-19'

aoi_name = 'bogo'
aois_path = join(DATA_DIR, 'polygons/aoi.geojson')

AOI_dict = get_aoi(json_path=aois_path, aoi_name=aoi_name)
AOI = gpd.GeoDataFrame.from_features([AOI_dict])

# define DEM filename inside data directory
DEM = f'{DATADIR}/dem.nc'



def pipe1_1(n_jobs=1):
    sbas = init_stack(
        dem=None,
        aoi=AOI,
        ref=REFERENCE,
        data_dir=DATADIR,
        work_dir=WORKDIR,
        verbose=True,
        drop_if_exists=False
    )

    print('Reframe')
    sbas.compute_reframe(AOI, n_jobs=n_jobs)
    
    sbas = None

def pipe1_2(n_jobs=1):
    sbas = init_stack(
        dem=DEM,
        aoi=AOI,
        ref=REFERENCE,
        data_dir=DATADIR,
        work_dir=WORKDIR,
        verbose=True,
        drop_if_exists=False
    )
    
    print('Aligment')
    sbas.compute_align(n_jobs=n_jobs)
    sbas = None
    
def pipe2(n_jobs=1):

    sbas = init_stack(
        dem=DEM,
        aoi=AOI,
        ref=REFERENCE,
        data_dir=DATADIR,
        work_dir=WORKDIR,
        verbose=True,
        drop_if_exists=False
    )
    
    print('Geocoding')
    # use the original Sentinel-1 resolution (1 pixel spacing)
    pixel_spacing = 1
    
    sbas.compute_trans(coarsen=pixel_spacing, n_jobs=n_jobs)
    sbas.compute_trans_inv(coarsen=pixel_spacing)
    
    sbas = None

def pipe3():
    sbas = init_stack(
        dem=DEM,
        aoi=AOI,
        ref=REFERENCE,
        data_dir=DATADIR,
        work_dir=WORKDIR,
        verbose=True,
        drop_if_exists=False
    )
    
    print('multilook')
    sbas.compute_satellite_look_vector()
    
    sbas = None
    

def pipe4():
    sbas = init_stack(
        dem=DEM,
        aoi=AOI,
        ref=REFERENCE,
        data_dir=DATADIR,
        work_dir=WORKDIR,
        verbose=True,
        drop_if_exists=False
    )
    
    print('compute PS')
    sbas.compute_ps()
    
    sbas = None
    

def pipe6():
    
    sbas = init_stack(
        dem=DEM,
        aoi=AOI,
        ref=REFERENCE,
        data_dir=DATADIR,
        work_dir=WORKDIR,
        verbose=True,
        drop_if_exists=False
    )
        
    baseline_pairs = sbas.sbas_pairs(days=60)
    
    sbas.compute_interferogram_multilook(baseline_pairs, 'intf_mlook', wavelength=200, psize=32, weight=sbas.psfunction(), queue=4)
    
    sbas = None
    
# Should be 7?  
def pipe7():
    
    sbas = init_stack(
        dem=DEM,
        aoi=AOI,
        ref=REFERENCE,
        data_dir=DATADIR,
        work_dir=WORKDIR,
        verbose=True,
        drop_if_exists=False
    )
        
    print('Generate Landmask')
    psmask_sbas = sbas.multilooking(sbas.psfunction(), coarsen=(1,4), wavelength=100) > 0.5
    topo_sbas = sbas.get_topo().interp_like(psmask_sbas, method='nearest')
    landmask_sbas = psmask_sbas&(np.isfinite(topo_sbas))
    landmask_sbas = utils.binary_opening(landmask_sbas, structure=np.ones((20,20)))
    landmask_sbas = np.isfinite(sbas.conncomp_main(landmask_sbas))
    landmask_sbas = utils.binary_closing(landmask_sbas, structure=np.ones((20,20)))
    landmask_sbas = np.isfinite(psmask_sbas.where(landmask_sbas))
    
    sbas = None
    return landmask_sbas
    #outdir = "/home/rav_marcin/projects/msg2sar/data/sar/sbas/desc/2023/bogo_pl_test/pickle"
    #outpath = os.path.join(outdir, 'landmask_sbas.pkl')
    #with open(outpath, 'wb') as f:  # open a text file
    #    pickle.dump(landmask_sbas, f) # serialize the list
    
    
def pipe8(landmask_sbas):
    sbas = init_stack(
        dem=DEM,
        aoi=AOI,
        ref=REFERENCE,
        data_dir=DATADIR,
        work_dir=WORKDIR,
        verbose=True,
        drop_if_exists=False
    )
        

    ds_sbas = sbas.open_stack('intf_mlook')
    
    # apply land mask
    ds_sbas = ds_sbas.where(landmask_sbas)
    
    intf_sbas = ds_sbas.phase
    corr_sbas = ds_sbas.correlation



print(1)


def start_client(n_workers=1, memory_limit="3.8GiB"):
    if 'dask_client' in globals():
        dask_client.close()
    
    dask_client = Client(n_workers=n_workers, memory_limit=memory_limit)
    return dask_client

def _process(n_workers: int, process: callable, kwargs: dict = None):
    dask_client = start_client(n_workers)
    if kwargs is not None:
        process(**kwargs)
    else:
        process()
    dask_client.close()


dask_client = start_client(1)


sbas = init_stack(
    dem=DEM,
    aoi=AOI,
    ref=REFERENCE,
    data_dir=DATADIR,
    work_dir=WORKDIR,
    verbose=True,
    drop_if_exists=False
)


psmask_sbas = sbas.multilooking(sbas.psfunction(), coarsen=(1,4), wavelength=100) > 0.5
topo_sbas = sbas.get_topo().interp_like(psmask_sbas, method='nearest')
landmask_sbas = psmask_sbas&(np.isfinite(topo_sbas))
landmask_sbas = utils.binary_opening(landmask_sbas, structure=np.ones((20,20)))
landmask_sbas = np.isfinite(sbas.conncomp_main(landmask_sbas))
landmask_sbas = utils.binary_closing(landmask_sbas, structure=np.ones((20,20)))
landmask_sbas = np.isfinite(psmask_sbas.where(landmask_sbas))


#dask_client.close()





landmask_sbas.to_netcdf("my_data_file.nc")


type(landmask_sbas)


landmask_sbas



